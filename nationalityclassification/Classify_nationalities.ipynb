{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbfyP2xM0CF7"
   },
   "source": [
    "# Experiments with models to derive nationality of Indian / Non-Indian from names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Dbc80CcKbZlE"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UomqHfGbfgTK"
   },
   "source": [
    "# Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NWlWNJlab1Bt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_data = pd.read_excel('male.xlsx')\n",
    "female_data = pd.read_excel('female.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_list = ['s/o','d/o','w/o','/','&',',','-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(name):\n",
    "    name = str(name).lower()\n",
    "    name = (''.join(i for i in name if ord(i)<128)).strip()\n",
    "    for repl in repl_list:\n",
    "        name = name.replace(repl,\" \")\n",
    "    if '@' in name:\n",
    "        pos = name.find('@')\n",
    "        name = name[:pos].strip()\n",
    "    name = name.split(\" \")\n",
    "    name = \" \".join([each.strip() for each in name])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "nlhlpq62d7Q5",
    "outputId": "463392d5-d57c-4422-dac0-618a3403255b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13754"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_records(merged_data):\n",
    "    merged_data['delete'] = 0\n",
    "    merged_data.loc[merged_data['name'].str.find('with') != -1,'delete'] = 1\n",
    "    merged_data.loc[merged_data['count_words']>=5,'delete']=1\n",
    "    merged_data.loc[merged_data['count_words']==0,'delete']=1\n",
    "    merged_data.loc[merged_data['name'].str.contains(r'\\d') == True,'delete']=1\n",
    "    cleaned_data = merged_data[merged_data.delete==0]\n",
    "    return cleaned_data\n",
    "\n",
    "merged_data = pd.concat((male_data,female_data),axis=0)\n",
    "\n",
    "merged_data['name'] = merged_data['name'].apply(clean_data)\n",
    "merged_data['count_words'] = merged_data['name'].str.split().apply(len)\n",
    "\n",
    "cleaned_data = remove_records(merged_data)\n",
    "\n",
    "indian_cleaned_data = cleaned_data[['name','count_words']].drop_duplicates(subset='name',keep='first')\n",
    "indian_cleaned_data['label'] = 'indian'\n",
    "\n",
    "len(indian_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "63XS_psZhKRI",
    "outputId": "8ecee80c-6730-4ff4-8301-775ec859cca0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>barjraj</td>\n",
       "      <td>1</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ramdin verma</td>\n",
       "      <td>2</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sharat chandran</td>\n",
       "      <td>2</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>birender mandal</td>\n",
       "      <td>2</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amit</td>\n",
       "      <td>1</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name  count_words   label\n",
       "0          barjraj            1  indian\n",
       "1     ramdin verma            2  indian\n",
       "2  sharat chandran            2  indian\n",
       "3  birender mandal            2  indian\n",
       "4             amit            1  indian"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indian_cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwYWD4NEiGW_"
   },
   "source": [
    "Lets create some non-Indian names using Faker - a package to generate realistic names from different regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "req = 15000\n",
    "non_indian_names = []\n",
    "\n",
    "langs = ['ar_EG','bs_BA','de_DE','dk_DK','en_AU','en_CA','en_GB',\n",
    "'en_IN','en_NZ','en_US','it_IT','no_NO','ro_RO']\n",
    "\n",
    "for i in range(0,req):\n",
    "    lng_indx = random.randint(0,len(langs)-1)\n",
    "    fake = Faker(langs[lng_indx])\n",
    "    non_indian_names.append(fake.name().lower())\n",
    "\n",
    "non_indian_names_orig = list(set(non_indian_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BmPIR4YqiJ_G",
    "outputId": "1cb817eb-f51d-4233-f87c-242a5a6819db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14547"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_indian_names_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ZMhbVP6_iPWY",
    "outputId": "5242d20c-b5cd-4430-a855-2c8860e25c25"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anna rivera</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>karl pearson</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ms alice simpson</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angela kelly</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>miss hannah roberts</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  count_words\n",
       "0          anna rivera            2\n",
       "1         karl pearson            2\n",
       "2     ms alice simpson            3\n",
       "3         angela kelly            2\n",
       "4  miss hannah roberts            3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_indian_data = pd.DataFrame({'name':non_indian_names_orig})\n",
    "non_indian_data['count_words'] = non_indian_data['name'].str.split().apply(len)\n",
    "non_indian_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhztvONSiUQx"
   },
   "source": [
    "Lets check the distribution of count of words in names. We dont want them to be too different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "53CFXZ45gJxW",
    "outputId": "ac75aed6-b376-4598-a0b4-b01bdb03759d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    7954\n",
       "1    4322\n",
       "3    1344\n",
       "4     134\n",
       "Name: count_words, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indian_cleaned_data['count_words'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "miobacqHihPl",
    "outputId": "906f4f3c-17bf-4377-cb2c-2069fb4ee33c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    13306\n",
       "3     1084\n",
       "4      154\n",
       "5        3\n",
       "Name: count_words, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_indian_data['count_words'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leg-a9-YipTD"
   },
   "source": [
    "We dont see any one word names at all, so lets just get some first names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "KiakWc3qim1M",
    "outputId": "89010055-05e2-4db8-eebe-75df4010a6b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    8306\n",
       "1    5000\n",
       "3    1084\n",
       "4     154\n",
       "Name: count_words, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_word_names = non_indian_data[non_indian_data['count_words']==2]['name']\n",
    "one_word_req = 5000\n",
    "names_one_two_words = [each.split()[0] for each in two_word_names[:one_word_req]] + list(two_word_names[one_word_req:])\n",
    "count_words = [1] * one_word_req + [2] * len(two_word_names[one_word_req:])\n",
    "not_two_words_pd  = non_indian_data[non_indian_data['count_words']!=2]\n",
    "one_two_words_pd = pd.DataFrame({'name':names_one_two_words,'count_words':count_words})\n",
    "non_indian_data = pd.concat((not_two_words_pd,one_two_words_pd),axis=0)\n",
    "non_indian_data['count_words'].value_counts()\n",
    "non_indian_data['label'] = 'non_indian'\n",
    "non_indian_data = non_indian_data[non_indian_data['count_words']<5]\n",
    "non_indian_data['count_words'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "xM5vFJSsi1nk",
    "outputId": "1d8fad22-3919-4524-8ac6-2292af58b6e8"
   },
   "outputs": [],
   "source": [
    "full_data = pd.concat((non_indian_data[['name','label']],indian_cleaned_data[['name','label']]),axis=0)\n",
    "full_data = full_data.sample(frac=1)\n",
    "\n",
    "full_data.to_csv(\"name_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_data = pd.read_csv('name_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyQAqpNdkgXz"
   },
   "source": [
    "# Get processed data files and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "H8UpNDKBlb7I",
    "outputId": "361f2760-fbda-4590-9cdc-10141abb7105"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>komal jaiswal</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>santlal</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dharmendra kumar</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shiv shakti singh</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jorj vargees</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name   label\n",
       "0      komal jaiswal  indian\n",
       "1            santlal  indian\n",
       "2   dharmendra kumar  indian\n",
       "3  shiv shakti singh  indian\n",
       "4       jorj vargees  indian"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "jzhmcUUplvbJ",
    "outputId": "2ecbd410-3f23-4eaa-e01b-844f96f66759"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non_indian    14544\n",
       "indian        13754\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "nIsGbp_-lzAt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = name_data['name'].astype(str)\n",
    "Y = name_data['label']\n",
    "train_names,test_names,train_labels,test_labels = train_test_split(X,Y,test_size=0.2,random_state =42,stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtUSR8DXmFN0"
   },
   "source": [
    "# Using Naive Bayes with Count Vectorizer for name classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\pooja\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pooja\\anaconda3\\lib\\site-packages (from sklearn) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\pooja\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\pooja\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pooja\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pooja\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "EWhFc1s9mAwx",
    "outputId": "b99a47bc-97f1-493c-91a6-2f9df278f20e"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11521"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_ = vectorizer.fit_transform(train_names.values.astype('U'))\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "hhHeUHOZmZSA",
    "outputId": "34d25615-ea01-4926-e039-0ab3fa5efe13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      indian       0.98      0.78      0.87      2751\n",
      "  non_indian       0.83      0.99      0.90      2909\n",
      "\n",
      "    accuracy                           0.89      5660\n",
      "   macro avg       0.91      0.89      0.89      5660\n",
      "weighted avg       0.90      0.89      0.89      5660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_,train_labels)\n",
    "\n",
    "X_test = vectorizer.transform(test_names.values.astype('U'))\n",
    "\n",
    "test_predicted = model.predict(X_test)\n",
    "\n",
    "print(classification_report(test_labels,test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEajV6-UqKdU"
   },
   "source": [
    "**Testing on new Names**\n",
    "<br>\n",
    "Lets curate some names which are not present in the data at all. And check the model on these names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "NcJ8xNaZmcIe",
    "outputId": "4825d934-79f4-4185-d12d-97669c6cc816"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>predictions_nb_cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lalitha</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tyson</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shailaja</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shyamala</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vishwanathan</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ramanujam</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>conan</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kryslovsky</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ratnani</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>diego</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kakoli</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>shreyas</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>brayden</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>shanon</td>\n",
       "      <td>non_indian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           names predictions_nb_cv\n",
       "0        lalitha        non_indian\n",
       "1          tyson        non_indian\n",
       "2       shailaja        non_indian\n",
       "3       shyamala        non_indian\n",
       "4   vishwanathan        non_indian\n",
       "5      ramanujam        non_indian\n",
       "6          conan        non_indian\n",
       "7     kryslovsky        non_indian\n",
       "8        ratnani        non_indian\n",
       "9          diego        non_indian\n",
       "10        kakoli        non_indian\n",
       "11       shreyas        non_indian\n",
       "12       brayden        non_indian\n",
       "13        shanon        non_indian"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_new_names = ['lalitha','tyson','shailaja','shyamala','vishwanathan','ramanujam','conan','kryslovsky',\n",
    "'ratnani','diego','kakoli','shreyas','brayden','shanon']\n",
    "\n",
    "X_new = vectorizer.transform(check_new_names)\n",
    "predictions_nb_cv = model.predict(X_new)\n",
    "test = pd.DataFrame({'names':check_new_names,'predictions_nb_cv':predictions_nb_cv})\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zvPUC93o_oB"
   },
   "source": [
    "Doesnt do well at all ! But thats expected. Now lets try with subword encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXlrL1LkqVqX"
   },
   "source": [
    "# Naive Bayes with SentencePiece Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "b9pn_A2Qmy1o",
    "outputId": "ee54aa5c-830c-43fd-bfa9-19ef77c6affe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "Installing collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.13.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pooja\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u0219' in position 8: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17576/3065719612.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_names.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u0219' in position 8: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "!pip3 install tokenizers\n",
    "from tokenizers import ByteLevelBPETokenizer,CharBPETokenizer,SentencePieceBPETokenizer,BertWordPieceTokenizer\n",
    "\n",
    "\n",
    "f = open(\"train_names.txt\",\"w\")\n",
    "for each in list(train_names):\n",
    "\tf.write(str(each))\n",
    "\tf.write(\"\\n\")\n",
    "\n",
    "f.close()\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train([\"./train_names.txt\"],vocab_size=2000,min_frequency=2)\n",
    "\n",
    "encoded_tokens = [tokenizer.encode(str(each)).tokens for each in train_names]\n",
    "encoded_tokens_test = [tokenizer.encode(str(each)).tokens for each in test_names]\n",
    "\n",
    "encoded_tokens = [\" \".join(each)  for each in encoded_tokens]\n",
    "encoded_tokens_test = [\" \".join(each)  for each in encoded_tokens_test]\n",
    "\n",
    "encoded_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "4Erx_vvLpu0H",
    "outputId": "c0273eac-033d-4796-d538-6441134751b4"
   },
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "X_ = tfidf_vect.fit_transform(encoded_tokens)\n",
    "len(tfidf_vect.get_feature_names())\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_,train_labels)\n",
    "\n",
    "X_test = tfidf_vect.transform(encoded_tokens_test)\n",
    "\n",
    "test_predicted = model.predict(X_test)\n",
    "\n",
    "print(classification_report(test_labels,test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNu-2bb1o_Ox"
   },
   "source": [
    "Pretty decent. Now lets check on some new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "AcF71VnTp2gg",
    "outputId": "c2f32d5b-ac60-46ef-b59f-933f783a12bc"
   },
   "outputs": [],
   "source": [
    "encoded_tokens_check = [tokenizer.encode(str(each).lower()).tokens for each in check_new_names]\n",
    "encoded_tokens_check = [\" \".join(each)  for each in encoded_tokens_check]\n",
    "\n",
    "X_new = tfidf_vect.transform(encoded_tokens_check)\n",
    "predictions_nb_enc_tf = model.predict(X_new)\n",
    "test = pd.DataFrame({'names':check_new_names,'predictions_nb_enc_tf':predictions_nb_enc_tf})\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIrMkFFUqhrs"
   },
   "source": [
    "# Lets also check with a Character based encoding with an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saA3yZWEqEPf"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import Callback\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuaX0K3JqxxQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def char_encoded_representation(data,tokenizer,vocab_size,max_len):\n",
    "\tchar_index_sentences = tokenizer.texts_to_sequences(data)\n",
    "\tsequences = [to_categorical(x, num_classes=vocab_size) for x in char_index_sentences]\n",
    "\tX = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "lB7BUz-tq4UT",
    "outputId": "21fe1559-88f1-4d02-a73b-e584212fbebd"
   },
   "outputs": [],
   "source": [
    "max_len = max([len(str(each)) for each in train_names])\n",
    "# mapping = get_char_mapping(train_names)\n",
    "# vocab_size = len(mapping)\n",
    "\n",
    "tok = Tokenizer(char_level=True)\n",
    "tok.fit_on_texts(train_names)\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "X_train = char_encoded_representation(train_names,tok,vocab_size,max_len)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qTqGCylxrpwj",
    "outputId": "18b4734b-8fe5-4a65-9232-a4cd219bc848"
   },
   "outputs": [],
   "source": [
    "X_test = char_encoded_representation(test_names,tok,vocab_size,max_len)\n",
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9-KFv-QrzdN"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "y_train = le.transform(train_labels)\n",
    "y_test = le.transform(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YW0rZXAFr2Bs"
   },
   "outputs": [],
   "source": [
    "# Model Specification\n",
    "\n",
    "\n",
    "def build_model(hidden_units,max_len,vocab_size):\n",
    "\tmodel = Sequential()\n",
    "\t# model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\tmodel.add(LSTM(hidden_units,input_shape=(max_len,vocab_size)))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\tprint(model.summary())\n",
    "\treturn model\n",
    "\n",
    "class myCallback(Callback):\n",
    "\tdef __init__(self,X_test,y_test):\n",
    "\t\tself.X_test = X_test\n",
    "\t\tself.y_test = y_test\n",
    "\tdef on_epoch_end(self, epoch, logs={}):\n",
    "\t\tloss,acc = model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "\t\tprint('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OEmIzQXssKZG",
    "outputId": "e24ed0aa-0725-4ad7-be5e-8d1f942f5b3e"
   },
   "outputs": [],
   "source": [
    "model = build_model(100,max_len,vocab_size)\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64,callbacks=myCallback(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "lhF4iG0GsQIn",
    "outputId": "497eabb6-e355-4ea3-b443-099b5280f3f9"
   },
   "outputs": [],
   "source": [
    "X_predict = char_encoded_representation(check_new_names,mapping)\n",
    "\n",
    "predictions_prob = model.predict(X_predict)\n",
    "predictions = np.array(predictions_prob)\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions = np.squeeze(predictions)\n",
    "predictions_lstm_char = le.inverse_transform(list(predictions.astype(int)))\n",
    "test = pd.DataFrame({'names':check_new_names,'predictions_lstm_char':predictions_lstm_char})\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63QcsQYJurHV"
   },
   "source": [
    "# SentencePiece Encoding with LSTM\n",
    "Lets also check with a encoding using the SentencePiece Encoding we used for Naive Bayes. But now we will use it with an LSTM with a much smaller vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnRNVUjIua9A"
   },
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer,CharBPETokenizer,SentencePieceBPETokenizer,BertWordPieceTokenizer\n",
    "vocab_size = 200\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train([\"./train_names.txt\"],vocab_size=vocab_size,min_frequency=2)\n",
    "\n",
    "\n",
    "def sent_piece_encoded_representation(data,tokenizer):\n",
    "\tencoded_tokens = [tokenizer.encode(str(each)).ids for each in data]\n",
    "\tsequences = [to_categorical(x, num_classes=vocab_size) for x in encoded_tokens]\n",
    "\tX = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "yYMi3uQhz5KC",
    "outputId": "8004d656-9787-461c-9dad-ee6dd04a4f61"
   },
   "outputs": [],
   "source": [
    "max_len = max([len(str(each)) for each in train_names])\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "y_train = le.transform(train_labels)\n",
    "y_test = le.transform(test_labels)\n",
    "\n",
    "\n",
    "X_train = sent_piece_encoded_representation(train_names,tokenizer)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "yuiv_nmv1YVy",
    "outputId": "a7c52603-eb72-467d-847a-ba3d8450a563"
   },
   "outputs": [],
   "source": [
    "X_test = sent_piece_encoded_representation(test_names,tokenizer)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b6D9PbO51jBq",
    "outputId": "ed6d22ba-c9bb-4aba-a31a-dec914bae1dd"
   },
   "outputs": [],
   "source": [
    "model = build_model(100,max_len,vocab_size)\n",
    "model.fit(X_train, y_train, epochs=12, batch_size=64,callbacks=myCallback(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "Ni8MkcTf1pzo",
    "outputId": "02335ab6-8e9d-4116-fad7-a6d75c81649a"
   },
   "outputs": [],
   "source": [
    "X_predict = sent_piece_encoded_representation(check_new_names,tokenizer)\n",
    "\n",
    "predictions_prob = model.predict(X_predict)\n",
    "predictions = np.array(predictions_prob)\n",
    "predictions[np.where(predictions > 0.5)[0]] = 1\n",
    "predictions[np.where(predictions <= 0.5)[0]] = 0\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "predictions_lstm_sent_enc = le.inverse_transform(list(predictions.astype(int)))\n",
    "test = pd.DataFrame({'names':check_new_names,'predictions_lstm_sent_enc':predictions_lstm_sent_enc})\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdbSyOOpMhMf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jyQAqpNdkgXz",
    "jtUSR8DXmFN0",
    "WXlrL1LkqVqX",
    "QIrMkFFUqhrs"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
